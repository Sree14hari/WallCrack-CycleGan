{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28678358",
   "metadata": {},
   "source": [
    "# WallCrack Using CycleGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5eac131",
   "metadata": {},
   "source": [
    "ðŸ“©**Downloading dataset from Source**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f82b30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\sreeh\\.cache\\kagglehub\\datasets\\arunrk7\\surface-crack-detection\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"arunrk7/surface-crack-detection\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b0571f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for images in: Dataset\n",
      "Found 20000 total images.\n",
      "Copying 500 random images to: Dataset_500\n",
      "\n",
      "âœ… Done. All files copied successfully.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "# --- 1. CONFIGURE YOUR FOLDERS AND SETTINGS HERE ---\n",
    "\n",
    "# The folder where all your original images are currently located.\n",
    "source_folder = Path(\"Dataset/\") \n",
    "\n",
    "# The new folder where you want the 500 images to be copied.\n",
    "destination_folder = Path(\"Dataset_500/\")\n",
    "\n",
    "# The number of images you want to select.\n",
    "num_to_select = 500\n",
    "\n",
    "# You can add other extensions if needed (e.g., \".gif\")\n",
    "image_extensions = [\".jpg\", \".jpeg\", \".png\"]\n",
    "\n",
    "\n",
    "# --- 2. THE SCRIPT ---\n",
    "\n",
    "try:\n",
    "    # Create the destination folder if it doesn't exist\n",
    "    destination_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"Searching for images in: {source_folder}\")\n",
    "\n",
    "    # Find all files that match the image extensions\n",
    "    all_images = [f for f in source_folder.glob(\"*\") if f.suffix.lower() in image_extensions]\n",
    "\n",
    "    if not all_images:\n",
    "        print(\"Error: No images found in the source folder.\")\n",
    "    else:\n",
    "        print(f\"Found {len(all_images)} total images.\")\n",
    "\n",
    "        # Make sure we don't try to select more images than are available\n",
    "        if len(all_images) < num_to_select:\n",
    "            print(f\"Warning: Only {len(all_images)} images available. Selecting all of them.\")\n",
    "            num_to_select = len(all_images)\n",
    "\n",
    "        # Randomly select the specified number of images\n",
    "        selected_images = random.sample(all_images, num_to_select)\n",
    "\n",
    "        print(f\"Copying {num_to_select} random images to: {destination_folder}\")\n",
    "\n",
    "        # Copy each selected file to the destination\n",
    "        for image_path in selected_images:\n",
    "            shutil.copy(image_path, destination_folder)\n",
    "\n",
    "        print(\"\\nâœ… Done. All files copied successfully.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The source folder was not found at '{source_folder}'\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730bcdcc",
   "metadata": {},
   "source": [
    "**Adding Blur**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b4c0278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 500 images. Starting blur process...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying blur: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:00<00:00, 634.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Done. All blurred images saved to 'trainA_blurred'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image, ImageFilter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 1. CONFIGURE YOUR SETTINGS HERE ---\n",
    "\n",
    "# Folder with your 500 clean, original images.\n",
    "source_folder = Path(\"Dataset_500/\")\n",
    "\n",
    "# Folder where the new blurred images will be saved (this will be part of trainA).\n",
    "destination_folder = Path(\"trainA_blurred/\")\n",
    "\n",
    "# Adjust the blur intensity. Higher numbers mean more blur. Try values between 2 and 8.\n",
    "blur_radius = 5\n",
    "\n",
    "# --- 2. THE SCRIPT ---\n",
    "\n",
    "try:\n",
    "    # Create the destination folder if it doesn't exist\n",
    "    destination_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Get a list of all image files in the source folder\n",
    "    image_files = list(source_folder.glob(\"*.jpg\")) + \\\n",
    "                  list(source_folder.glob(\"*.jpeg\")) + \\\n",
    "                  list(source_folder.glob(\"*.png\"))\n",
    "\n",
    "    if not image_files:\n",
    "        print(f\"Error: No images found in '{source_folder}'\")\n",
    "    else:\n",
    "        print(f\"Found {len(image_files)} images. Starting blur process...\")\n",
    "\n",
    "        # Loop through all images and show a progress bar\n",
    "        for image_path in tqdm(image_files, desc=\"Applying blur\"):\n",
    "            try:\n",
    "                # Open the image\n",
    "                with Image.open(image_path) as img:\n",
    "                    # Apply a Gaussian blur filter\n",
    "                    blurred_img = img.filter(ImageFilter.GaussianBlur(radius=blur_radius))\n",
    "                    \n",
    "                    # Construct the new filename/path for the blurred image\n",
    "                   \n",
    "                    save_path = destination_folder / f\"blur_{image_path.name}\"\n",
    "                    # Save the blurred image\n",
    "                    blurred_img.save(save_path)\n",
    "            except Exception as e:\n",
    "                print(f\"\\nCould not process {image_path.name}: {e}\")\n",
    "\n",
    "        print(f\"\\nâœ… Done. All blurred images saved to '{destination_folder}'.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Source folder not found at '{source_folder}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86ecced",
   "metadata": {},
   "source": [
    "**Adding Shadow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ddfcf74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 500 images. Adding shadows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding shadows: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:02<00:00, 225.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Done. All shadowed images saved to 'trainA_shadowed'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 1. CONFIGURE YOUR SETTINGS HERE ---\n",
    "\n",
    "# Folder with your 500 clean, original images.\n",
    "source_folder = Path(\"Dataset_500/\")\n",
    "\n",
    "# Folder where the new shadowed images will be saved.\n",
    "destination_folder = Path(\"trainA_shadowed/\")\n",
    "\n",
    "# --- 2. THE SCRIPT ---\n",
    "\n",
    "def add_shadow(image):\n",
    "    \"\"\"Adds a random shadow effect to an image.\"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "    \n",
    "    # Create a completely black mask\n",
    "    mask = np.zeros_like(image, dtype=np.float32)\n",
    "    \n",
    "    # --- Define a random polygon for the shadow ---\n",
    "    # Choose a random corner to start the shadow from (0: top-left, 1: top-right)\n",
    "    start_corner = random.choice([0, 1])\n",
    "    \n",
    "    if start_corner == 0:\n",
    "        # Shadow from top-left\n",
    "        points = np.array([\n",
    "            [0, 0],\n",
    "            [random.randint(w//2, w), 0],\n",
    "            [random.randint(0, w//2), h]\n",
    "        ], dtype=np.int32)\n",
    "    else:\n",
    "        # Shadow from top-right\n",
    "        points = np.array([\n",
    "            [w, 0],\n",
    "            [random.randint(0, w//2), 0],\n",
    "            [random.randint(w//2, w), h]\n",
    "        ], dtype=np.int32)\n",
    "\n",
    "    # Draw the white polygon on the black mask\n",
    "    cv2.fillPoly(mask, [points], (1, 1, 1))\n",
    "    \n",
    "    # --- Make the shadow realistic ---\n",
    "    # Add a strong blur to the mask to create soft edges\n",
    "    # The kernel size must be odd\n",
    "    blur_kernel_size = random.randrange(101, 251, 2)\n",
    "    mask = cv2.GaussianBlur(mask, (blur_kernel_size, blur_kernel_size), 0)\n",
    "    \n",
    "    # Control the shadow's darkness (0.5=dark, 0.8=light)\n",
    "    shadow_intensity = random.uniform(0.5, 0.8)\n",
    "    \n",
    "    # Blend the shadow mask with the original image\n",
    "    # We convert image to float for multiplication, then back to uint8\n",
    "    shadowed_image = image.astype(np.float32) * (1 - (mask * shadow_intensity))\n",
    "    shadowed_image = np.clip(shadowed_image, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    return shadowed_image\n",
    "\n",
    "# --- Main script logic ---\n",
    "try:\n",
    "    destination_folder.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    image_files = list(source_folder.glob(\"*.jpg\")) + \\\n",
    "                  list(source_folder.glob(\"*.jpeg\")) + \\\n",
    "                  list(source_folder.glob(\"*.png\"))\n",
    "\n",
    "    if not image_files:\n",
    "        print(f\"Error: No images found in '{source_folder}'\")\n",
    "    else:\n",
    "        print(f\"Found {len(image_files)} images. Adding shadows...\")\n",
    "\n",
    "        for image_path in tqdm(image_files, desc=\"Adding shadows\"):\n",
    "            try:\n",
    "                # Read image using OpenCV\n",
    "                img = cv2.imread(str(image_path))\n",
    "                if img is None:\n",
    "                    print(f\"\\nCould not read {image_path.name}\")\n",
    "                    continue\n",
    "                \n",
    "                # Add the shadow\n",
    "                shadowed_img = add_shadow(img)\n",
    "                \n",
    "                # Save the new image\n",
    "                save_path = destination_folder / f\"shadow_{image_path.name}\"\n",
    "                cv2.imwrite(str(save_path), shadowed_img)\n",
    "            except Exception as e:\n",
    "                print(f\"\\nCould not process {image_path.name}: {e}\")\n",
    "        \n",
    "        print(f\"\\nâœ… Done. All shadowed images saved to '{destination_folder}'.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Source folder not found at '{source_folder}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0d6459",
   "metadata": {},
   "source": [
    "**Add Both Blur And Shadow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b12e548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 500 images. Applying shadow and blur...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:02<00:00, 213.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Done. All images with shadow and blur saved to 'trainA_blur_and_shadow'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "# --- 1. CONFIGURE YOUR SETTINGS HERE ---\n",
    "\n",
    "# Folder with your 500 clean, original images.\n",
    "source_folder = Path(\"Dataset_500/\")\n",
    "\n",
    "# Folder where the new combined-effect images will be saved.\n",
    "destination_folder = Path(\"trainA_blur_and_shadow/\")\n",
    "\n",
    "# The kernel size for the blur. Must be a pair of odd numbers. (e.g., (9, 9))\n",
    "blur_kernel_size = (9, 9)\n",
    "\n",
    "\n",
    "# --- 2. HELPER FUNCTION (same as before) ---\n",
    "\n",
    "def add_shadow(image):\n",
    "    \"\"\"Adds a random shadow effect to an image.\"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "    mask = np.zeros_like(image, dtype=np.float32)\n",
    "    start_corner = random.choice([0, 1])\n",
    "    \n",
    "    if start_corner == 0: # Top-left\n",
    "        points = np.array([[0, 0], [random.randint(w//2, w), 0], [random.randint(0, w//2), h]], dtype=np.int32)\n",
    "    else: # Top-right\n",
    "        points = np.array([[w, 0], [random.randint(0, w//2), 0], [random.randint(w//2, w), h]], dtype=np.int32)\n",
    "\n",
    "    cv2.fillPoly(mask, [points], (1, 1, 1))\n",
    "    blur_k = random.randrange(101, 251, 2)\n",
    "    mask = cv2.GaussianBlur(mask, (blur_k, blur_k), 0)\n",
    "    shadow_intensity = random.uniform(0.5, 0.8)\n",
    "    shadowed_image = image.astype(np.float32) * (1 - (mask * shadow_intensity))\n",
    "    return np.clip(shadowed_image, 0, 255).astype(np.uint8)\n",
    "\n",
    "\n",
    "# --- 3. MAIN SCRIPT LOGIC ---\n",
    "\n",
    "try:\n",
    "    destination_folder.mkdir(parents=True, exist_ok=True)\n",
    "    image_files = list(source_folder.glob(\"*.jpg\")) + list(source_folder.glob(\"*.png\"))\n",
    "\n",
    "    if not image_files:\n",
    "        print(f\"Error: No images found in '{source_folder}'\")\n",
    "    else:\n",
    "        print(f\"Found {len(image_files)} images. Applying shadow and blur...\")\n",
    "\n",
    "        for image_path in tqdm(image_files, desc=\"Processing images\"):\n",
    "            try:\n",
    "                img = cv2.imread(str(image_path))\n",
    "                shadowed_img = add_shadow(img)\n",
    "                final_img = cv2.GaussianBlur(shadowed_img, blur_kernel_size, 0)\n",
    "                save_path = destination_folder / f\"both_{image_path.name}\"\n",
    "                cv2.imwrite(str(save_path), final_img)\n",
    "            except Exception as e:\n",
    "                print(f\"\\nCould not process {image_path.name}: {e}\")\n",
    "        \n",
    "        print(f\"\\nâœ… Done. All images with shadow and blur saved to '{destination_folder}'.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Source folder not found at '{source_folder}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e3c7d9",
   "metadata": {},
   "source": [
    "Combining Dataset For Train A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "574d462b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assembling final 'trainA' folder...\n",
      "- Copying files from trainA_blurred...\n",
      "- Copying files from trainA_shadowed...\n",
      "- Copying files from trainA_blur_and_shadow...\n",
      "\n",
      "âœ… All augmented images have been combined into 'trainA'.\n",
      "You are now ready to prepare your 'trainB' folder and start training!\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# List of folders containing your augmented images\n",
    "source_folders = [\n",
    "    Path(\"trainA_blurred/\"),\n",
    "    Path(\"trainA_shadowed/\"),\n",
    "    Path(\"trainA_blur_and_shadow/\")\n",
    "]\n",
    "\n",
    "# The final destination folder for your CycleGAN\n",
    "final_trainA_folder = Path(\"trainA/\")\n",
    "\n",
    "# --- Script to combine folders ---\n",
    "final_trainA_folder.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Assembling final 'trainA' folder...\")\n",
    "\n",
    "for folder in source_folders:\n",
    "    print(f\"- Copying files from {folder}...\")\n",
    "    if folder.is_dir():\n",
    "        for file in folder.iterdir():\n",
    "            shutil.copy(file, final_trainA_folder)\n",
    "    else:\n",
    "        print(f\"  Warning: Folder not found.\")\n",
    "\n",
    "print(\"\\nâœ… All augmented images have been combined into 'trainA'.\")\n",
    "print(\"You are now ready to prepare your 'trainB' folder and start training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48044e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking image count in: trainA\n",
      "\n",
      "âœ… The folder 'trainA' contains: 1500 images.\n",
      "The count is exactly 1500. Your trainA dataset is ready!\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# --- 1. Set the folder you want to count ---\n",
    "folder_to_check = Path(\"trainA/\")\n",
    "\n",
    "# --- You can customize these if needed ---\n",
    "image_extensions = [\".jpg\", \".jpeg\", \".png\"]\n",
    "\n",
    "# --- 2. The script to count files ---\n",
    "print(f\"Checking image count in: {folder_to_check}\\n\")\n",
    "\n",
    "if not folder_to_check.is_dir():\n",
    "    print(f\"Error: Directory not found at '{folder_to_check}'\")\n",
    "else:\n",
    "    # Count all files with the specified image extensions\n",
    "    count = sum(1 for f in folder_to_check.iterdir() if f.suffix.lower() in image_extensions)\n",
    "    \n",
    "    print(f\"âœ… The folder '{folder_to_check}' contains: {count} images.\")\n",
    "\n",
    "    # Check if the count matches the expected total\n",
    "    if count == 1500:\n",
    "        print(\"The count is exactly 1500. Your trainA dataset is ready!\")\n",
    "    else:\n",
    "        print(f\"Warning: The count is {count}, not 1500. You may want to re-run the final copy script.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25e9258c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting 500 images into:\n",
      "- 400 for trainB\n",
      "- 100 for testB\n",
      "\n",
      "âœ… Done. trainB and testB folders created successfully.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Configuration ---\n",
    "source_folder = Path(\"Dataset_500/\")\n",
    "train_b_folder = Path(\"trainB/\")\n",
    "test_b_folder = Path(\"testB/\")\n",
    "train_split_ratio = 0.8  # 80% for training, 20% for testing\n",
    "\n",
    "# --- Script ---\n",
    "try:\n",
    "    # Create destination folders\n",
    "    train_b_folder.mkdir(parents=True, exist_ok=True)\n",
    "    test_b_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Get all image files\n",
    "    image_files = list(source_folder.glob(\"*.jpg\")) + list(source_folder.glob(\"*.png\"))\n",
    "    random.shuffle(image_files) # Shuffle for a random split\n",
    "\n",
    "    # Determine the split index\n",
    "    split_index = int(len(image_files) * train_split_ratio)\n",
    "\n",
    "    # Split the list into training and testing sets\n",
    "    train_files = image_files[:split_index]\n",
    "    test_files = image_files[split_index:]\n",
    "\n",
    "    print(f\"Splitting {len(image_files)} images into:\")\n",
    "    print(f\"- {len(train_files)} for trainB\")\n",
    "    print(f\"- {len(test_files)} for testB\")\n",
    "\n",
    "    # Copy files\n",
    "    for f in train_files:\n",
    "        shutil.copy(f, train_b_folder)\n",
    "    for f in test_files:\n",
    "        shutil.copy(f, test_b_folder)\n",
    "\n",
    "    print(\"\\nâœ… Done. trainB and testB folders created successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da2392dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented test set 'testA' from 100 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating testA: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 121.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Done. 'testA' created with 300 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Configuration ---\n",
    "source_folder = Path(\"testB/\") # Source is the clean test images\n",
    "destination_folder = Path(\"testA/\") # Destination for augmented test images\n",
    "blur_kernel_size = (9, 9)\n",
    "\n",
    "# --- Helper function for shadows ---\n",
    "def add_shadow(image):\n",
    "    h, w = image.shape[:2]\n",
    "    mask = np.zeros_like(image, dtype=np.float32)\n",
    "    start_corner = random.choice([0, 1])\n",
    "    if start_corner == 0:\n",
    "        points = np.array([[0, 0], [random.randint(w//2, w), 0], [random.randint(0, w//2), h]], dtype=np.int32)\n",
    "    else:\n",
    "        points = np.array([[w, 0], [random.randint(0, w//2), 0], [random.randint(w//2, w), h]], dtype=np.int32)\n",
    "    cv2.fillPoly(mask, [points], (1, 1, 1))\n",
    "    blur_k = random.randrange(101, 251, 2)\n",
    "    mask = cv2.GaussianBlur(mask, (blur_k, blur_k), 0)\n",
    "    shadow_intensity = random.uniform(0.5, 0.8)\n",
    "    return np.clip(image.astype(np.float32) * (1 - (mask * shadow_intensity)), 0, 255).astype(np.uint8)\n",
    "\n",
    "# --- Main Script ---\n",
    "try:\n",
    "    destination_folder.mkdir(parents=True, exist_ok=True)\n",
    "    image_files = list(source_folder.glob(\"*.jpg\")) + list(source_folder.glob(\"*.png\"))\n",
    "\n",
    "    print(f\"Generating augmented test set 'testA' from {len(image_files)} images...\")\n",
    "\n",
    "    for image_path in tqdm(image_files, desc=\"Creating testA\"):\n",
    "        img = cv2.imread(str(image_path))\n",
    "        \n",
    "        # 1. Create and save blurred version\n",
    "        blurred_img = cv2.GaussianBlur(img, blur_kernel_size, 0)\n",
    "        cv2.imwrite(str(destination_folder / f\"blur_{image_path.name}\"), blurred_img)\n",
    "\n",
    "        # 2. Create and save shadowed version\n",
    "        shadowed_img = add_shadow(img)\n",
    "        cv2.imwrite(str(destination_folder / f\"shadow_{image_path.name}\"), shadowed_img)\n",
    "        \n",
    "        # 3. Create and save combined version (shadow then blur)\n",
    "        final_img = cv2.GaussianBlur(shadowed_img, blur_kernel_size, 0)\n",
    "        cv2.imwrite(str(destination_folder / f\"both_{image_path.name}\"), final_img)\n",
    "        \n",
    "    print(f\"\\nâœ… Done. 'testA' created with {len(list(destination_folder.glob('*')))} images.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7513687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing 100 ground truth images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:01<00:00, 83.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation Results ---\n",
      "âœ… Average PSNR: 25.11 dB\n",
      "âœ… Average SSIM: 0.8903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 1. CONFIGURE YOUR FOLDERS HERE ---\n",
    "\n",
    "# Path to your original clean test images (the ground truth)\n",
    "ground_truth_folder = Path(\"D:/Machine Learning/WallCrack_CG/Data/testB/\")\n",
    "\n",
    "# Path to the images generated by the test.py script\n",
    "predicted_folder = Path(r\"D:\\Machine Learning\\WallCrack_CG\\pytorch-CycleGAN-and-pix2pix\\results\\crack_wall_cleanup_model\\test_50\\images\")\n",
    "\n",
    "# --- 2. THE SCRIPT (FINAL VERSION) ---\n",
    "psnr_scores = []\n",
    "ssim_scores = []\n",
    "\n",
    "# Get the list of original clean images\n",
    "ground_truth_files = sorted(list(ground_truth_folder.glob(\"*.jpg\"))) + \\\n",
    "                     sorted(list(ground_truth_folder.glob(\"*.png\")))\n",
    "\n",
    "print(f\"Comparing {len(ground_truth_files)} ground truth images...\")\n",
    "\n",
    "for gt_path in tqdm(ground_truth_files, desc=\"Calculating Metrics\"):\n",
    "    base_name = gt_path.stem\n",
    "    \n",
    "    # --- THIS IS THE CORRECTED LINE ---\n",
    "    predicted_path = next(predicted_folder.glob(f\"*_{base_name}_fake.png\"), None)\n",
    "\n",
    "    if predicted_path is None:\n",
    "        print(f\"\\nWarning: No corresponding prediction found for {gt_path.name}\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Load images in grayscale for standard calculation\n",
    "        img_gt = cv2.imread(str(gt_path), cv2.IMREAD_GRAYSCALE)\n",
    "        img_pred = cv2.imread(str(predicted_path), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # Ensure images are the same size\n",
    "        if img_gt.shape != img_pred.shape:\n",
    "            h, w = img_gt.shape\n",
    "            img_pred = cv2.resize(img_pred, (w, h))\n",
    "\n",
    "        # Calculate metrics\n",
    "        psnr_scores.append(psnr(img_gt, img_pred, data_range=255))\n",
    "        ssim_scores.append(ssim(img_gt, img_pred, data_range=255))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError processing {gt_path.name}: {e}\")\n",
    "\n",
    "# Calculate and print the average scores\n",
    "if psnr_scores and ssim_scores:\n",
    "    avg_psnr = np.mean(psnr_scores)\n",
    "    avg_ssim = np.mean(ssim_scores)\n",
    "\n",
    "    print(\"\\n--- Evaluation Results ---\")\n",
    "    print(f\"âœ… Average PSNR: {avg_psnr:.2f} dB\")\n",
    "    print(f\"âœ… Average SSIM: {avg_ssim:.4f}\")\n",
    "else:\n",
    "    print(\"\\nNo images were successfully processed. Please check folder paths and filenames.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dde47cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing all test images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:01<00:00, 57.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Crack Width Analysis (in pixels) ---\n",
      "  filename      clean       blur  restored_blur     shadow  restored_shadow  \\\n",
      "0    00343  10.117659  10.914913      10.288900  33.172344        11.366459   \n",
      "1    00741   3.835974   3.769678       3.878775  68.316444         6.237106   \n",
      "2    00786   6.742176   6.834746       7.202435  54.824524         7.287296   \n",
      "3    01420  15.785818  16.339111      15.449415  51.647896        17.587482   \n",
      "4    01474  11.130548  12.265622      10.631347  18.486263        11.060898   \n",
      "\n",
      "        both  restored_both  \n",
      "0  35.023777      11.349206  \n",
      "1  77.179230      10.466269  \n",
      "2  57.933693       7.345812  \n",
      "3  52.986858      17.953011  \n",
      "4  26.166599      11.222775  \n",
      "\n",
      "âœ… Full analysis saved to 'crack_width_analysis_epoch50.csv'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def measure_crack_width(image_path):\n",
    "    \"\"\"Measures the average width of a crack in an image.\"\"\"\n",
    "    if not Path(image_path).exists():\n",
    "        return 0\n",
    "    image = cv2.imread(str(image_path), cv2.IMREAD_GRAYSCALE)\n",
    "    threshold_value = 100\n",
    "    _, mask = cv2.threshold(image, threshold_value, 255, cv2.THRESH_BINARY_INV)\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    cleaned_mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel, iterations=2)\n",
    "    dist_transform = cv2.distanceTransform(cleaned_mask, cv2.DIST_L2, 5)\n",
    "    crack_pixels = dist_transform[cleaned_mask > 0]\n",
    "    if len(crack_pixels) == 0:\n",
    "        return 0\n",
    "    return np.mean(crack_pixels) * 2\n",
    "\n",
    "# --- 1. CONFIGURE YOUR FOLDER PATHS ---\n",
    "test_b_folder = Path(\"D:/Machine Learning/WallCrack_CG/Data/testB/\")\n",
    "test_a_folder = Path(\"D:/Machine Learning/WallCrack_CG/Data/testA/\")\n",
    "# --- IMPORTANT: Point this to your final epoch 50 results ---\n",
    "results_folder = Path(r\"D:\\Machine Learning\\WallCrack_CG\\pytorch-CycleGAN-and-pix2pix\\results\\crack_wall_cleanup_model\\test_50\\images\")\n",
    "\n",
    "# --- 2. THE AUTOMATED SCRIPT ---\n",
    "results_data = []\n",
    "clean_files = sorted(list(test_b_folder.glob(\"*.jpg\"))) + sorted(list(test_b_folder.glob(\"*.png\")))\n",
    "\n",
    "for clean_path in tqdm(clean_files, desc=\"Processing all test images\"):\n",
    "    base_name = clean_path.stem\n",
    "    \n",
    "    paths = {\n",
    "        'clean': clean_path,\n",
    "        'blur': test_a_folder / f\"blur_{base_name}.jpg\",\n",
    "        'shadow': test_a_folder / f\"shadow_{base_name}.jpg\",\n",
    "        'both': test_a_folder / f\"both_{base_name}.jpg\",\n",
    "        'restored_blur': results_folder / f\"blur_{base_name}_fake.png\",\n",
    "        'restored_shadow': results_folder / f\"shadow_{base_name}_fake.png\",\n",
    "        'restored_both': results_folder / f\"both_{base_name}_fake.png\"\n",
    "    }\n",
    "    \n",
    "    widths = {key: measure_crack_width(path) for key, path in paths.items()}\n",
    "    widths['filename'] = base_name\n",
    "    results_data.append(widths)\n",
    "\n",
    "# --- 3. CREATE AND SAVE THE REPORT ---\n",
    "df = pd.DataFrame(results_data)\n",
    "df = df[['filename', 'clean', 'blur', 'restored_blur', 'shadow', 'restored_shadow', 'both', 'restored_both']]\n",
    "\n",
    "print(\"\\n--- Crack Width Analysis (in pixels) ---\")\n",
    "print(df.head())\n",
    "\n",
    "output_csv_path = \"crack_width_analysis_epoch50.csv\"\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"\\nâœ… Full analysis saved to '{output_csv_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f01c43a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing testB: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 369.22it/s]\n",
      "Analyzing testA: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:01<00:00, 228.89it/s]\n",
      "Analyzing images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 600/600 [00:06<00:00, 99.20it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Comparison Report ---\n",
      "  base_filename augmentation_type  clean_width_mm   clean_class  \\\n",
      "0         00343              blur        2.800000   Heavy Crack   \n",
      "1         00343            shadow        2.800000   Heavy Crack   \n",
      "2         00343              both        2.800000   Heavy Crack   \n",
      "3         00741              blur        1.216553  Medium Crack   \n",
      "4         00741            shadow        1.216553  Medium Crack   \n",
      "5         00741              both        1.216553  Medium Crack   \n",
      "\n",
      "   augmented_width_mm augmented_class  restored_width_mm restored_class  \n",
      "0            2.973214     Heavy Crack           2.952965    Heavy Crack  \n",
      "1           11.014536     Heavy Crack           3.124100    Heavy Crack  \n",
      "2           11.200000     Heavy Crack           3.104835    Heavy Crack  \n",
      "3            1.264911    Medium Crack           1.166190   Medium Crack  \n",
      "4           11.343721     Heavy Crack           5.099020    Heavy Crack  \n",
      "5           13.441726     Heavy Crack           6.794115    Heavy Crack  \n",
      "\n",
      "âœ… Final comparison report saved to 'final_crack_width_comparison.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from skimage.morphology import skeletonize\n",
    "from scipy.ndimage import distance_transform_edt\n",
    "from zipfile import ZipFile\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ===============================================================\n",
    "# ===== 1. PASTE ALL OF YOUR PROFESSOR'S FUNCTIONS HERE =========\n",
    "# ===============================================================\n",
    "\n",
    "def classify_crack_width(width_mm):\n",
    "    if width_mm <= 1.0:\n",
    "        return \"Light Crack\"\n",
    "    elif width_mm <= 2.0:\n",
    "        return \"Medium Crack\"\n",
    "    else:\n",
    "        return \"Heavy Crack\"\n",
    "\n",
    "def crack_width_measure(binary_image, pixel_size_mm=0.1, display_results=False):\n",
    "    if binary_image.dtype not in [np.uint8, np.float32, np.float64]:\n",
    "        binary_image = binary_image.astype(np.uint8)\n",
    "\n",
    "    if np.sum(binary_image) == 0:\n",
    "        return np.array([0]), 0\n",
    "\n",
    "    skeleton = skeletonize(binary_image > 0)\n",
    "    distance = distance_transform_edt(binary_image)\n",
    "    crack_widths_pixels = distance[skeleton]\n",
    "    if len(crack_widths_pixels) == 0:\n",
    "        return np.array([0]), 0\n",
    "        \n",
    "    crack_widths_pixels = crack_widths_pixels * 2\n",
    "    crack_widths_mm = crack_widths_pixels * pixel_size_mm\n",
    "    \n",
    "    if display_results:\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.imshow(binary_image, cmap='gray')\n",
    "        y, x = np.where(skeleton)\n",
    "        sc = plt.scatter(x, y, c=crack_widths_mm, cmap='jet', s=10)\n",
    "        plt.colorbar(sc, label='Crack Width (mm)')\n",
    "        plt.title('Crack Width Measurement (mm)')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    return crack_widths_mm, np.max(crack_widths_mm)\n",
    "\n",
    "def analyse_crack_image(image_path, pixel_size_mm=0.1, display_results=False):\n",
    "    img = cv2.imread(str(image_path), cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        return np.array([0]), 0, \"Error\"\n",
    "    _, binary_image = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY_INV)\n",
    "    crack_widths_mm, max_crack_width_mm = crack_width_measure(binary_image, pixel_size_mm, display_results)\n",
    "    crack_classification = classify_crack_width(max_crack_width_mm)\n",
    "    return crack_widths_mm, max_crack_width_mm, crack_classification\n",
    "\n",
    "def analyse_folder(folder_path, output_folder, pixel_size_mm=0.1, display_results=False):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    results = {}\n",
    "    \n",
    "    image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "    for filename in tqdm(image_files, desc=f\"Analyzing {Path(folder_path).name}\"):\n",
    "        image_path = os.path.join(folder_path, filename)\n",
    "        _, max_width, classification = analyse_crack_image(image_path, pixel_size_mm, display_results)\n",
    "        results[filename] = (max_width, classification)\n",
    "    return results, None # Simplified to not create folders/zip\n",
    "\n",
    "# ===============================================================\n",
    "# =========== 2. DEFINE YOUR FOLDERS AND RUN ANALYSIS ===========\n",
    "# ===============================================================\n",
    "\n",
    "# --- Define Your Folders ---\n",
    "folder_clean = Path(\"D:/Machine Learning/WallCrack_CG/Data/testB/\")\n",
    "folder_augmented = Path(\"D:/Machine Learning/WallCrack_CG/Data/testA/\")\n",
    "folder_restored = Path(r\"D:\\Machine Learning\\WallCrack_CG\\pytorch-CycleGAN-and-pix2pix\\results\\crack_wall_cleanup_model\\test_50\\images\")\n",
    "output_base = Path(\"./crack_analysis_results\")\n",
    "\n",
    "# --- Run Analysis on Each Folder ---\n",
    "results_clean, _ = analyse_folder(str(folder_clean), str(output_base / \"clean\"))\n",
    "results_augmented, _ = analyse_folder(str(folder_augmented), str(output_base / \"augmented\"))\n",
    "results_restored, _ = analyse_folder(str(folder_restored), str(output_base / \"restored\"))\n",
    "\n",
    "# --- 3. COMBINE AND COMPARE THE RESULTS ---\n",
    "comparison_data = []\n",
    "\n",
    "# Loop through the clean images as the ground truth\n",
    "for clean_filename, (clean_width, clean_class) in results_clean.items():\n",
    "    base_name = Path(clean_filename).stem\n",
    "\n",
    "    for prefix in ['blur', 'shadow', 'both']:\n",
    "        aug_filename = f\"{prefix}_{base_name}.jpg\"\n",
    "        res_filename = f\"{prefix}_{base_name}_fake.png\"\n",
    "        \n",
    "        aug_width, aug_class = results_augmented.get(aug_filename, (None, \"N/A\"))\n",
    "        res_width, res_class = results_restored.get(res_filename, (None, \"N/A\"))\n",
    "        \n",
    "        comparison_data.append({\n",
    "            'base_filename': base_name,\n",
    "            'augmentation_type': prefix,\n",
    "            'clean_width_mm': clean_width,\n",
    "            'clean_class': clean_class,\n",
    "            'augmented_width_mm': aug_width,\n",
    "            'augmented_class': aug_class,\n",
    "            'restored_width_mm': res_width,\n",
    "            'restored_class': res_class\n",
    "        })\n",
    "\n",
    "# Create a final DataFrame\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"\\n--- Final Comparison Report ---\")\n",
    "print(df_comparison.head(6))\n",
    "\n",
    "# Save the final report to a CSV\n",
    "df_comparison.to_csv(\"final_crack_width_comparison.csv\", index=False)\n",
    "print(\"\\nâœ… Final comparison report saved to 'final_crack_width_comparison.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
